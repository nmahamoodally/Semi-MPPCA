{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l8eHHmGehkAv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import pickle\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    ConfusionMatrixDisplay,\n",
        "    precision_recall_fscore_support,\n",
        "    roc_auc_score\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ze0de3jhhkBD"
      },
      "outputs": [],
      "source": [
        "#cluster-label mapping\n",
        "def match_clusters(pred_labels, true_labels):\n",
        "    n_clusters = len(np.unique(true_labels))  # Assume the number of clusters is known\n",
        "    confusion_matrix = np.zeros((n_clusters, n_clusters), dtype=int)\n",
        "\n",
        "    for i in range(len(true_labels)):\n",
        "        # Ensure indices are integers by converting them to integers\n",
        "        row_index = int(true_labels[i])\n",
        "        col_index = int(pred_labels[i])\n",
        "\n",
        "        # Access the confusion matrix using the integer indices\n",
        "        confusion_matrix[row_index, col_index] += 1  # Count occurrences\n",
        "\n",
        "    # Solve optimal assignment problem\n",
        "    row_ind, col_ind = linear_sum_assignment(confusion_matrix.max() - confusion_matrix)\n",
        "\n",
        "    # Create mapping\n",
        "    cluster_mapping = {col: row for row, col in zip(row_ind, col_ind)}\n",
        "\n",
        "    # Relabel predicted clusters using the mapping\n",
        "    matched_pred_labels = np.array([cluster_mapping[label] for label in pred_labels])\n",
        "\n",
        "    return matched_pred_labels, cluster_mapping\n",
        "\n",
        "\n",
        "#Generates a random batch of data from the input dataset.\n",
        "def random_batch(X, y=None, batch_size=32):\n",
        "    idx = np.random.randint(len(X), size=batch_size)\n",
        "    return X[idx]\n",
        "\n",
        "#Creates a visual progress bar as a string to track the progress of a process.\n",
        "def progress_bar(iteration, total, size=30):\n",
        "    running = iteration < total\n",
        "    c = \">\" if running else \"=\"\n",
        "    p = (size - 1) * iteration // total\n",
        "    fmt = \"{{:-{}d}}/{{}} [{{}}]\".format(len(str(total)))\n",
        "    params = [iteration, total, \"=\" * p + c + \".\" * (size - p - 1)]\n",
        "    return fmt.format(*params)\n",
        "\n",
        "#Displays the progress bar along with the loss and other metrics in real time.\n",
        "def print_status_bar(iteration, total, loss, metrics=None, size=30):\n",
        "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result()) for m in [loss] + (metrics or [])])\n",
        "    end = \"\" if iteration < total else \"\\n\"\n",
        "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)\n",
        "\n",
        "#Calculates the Euclidean norm (L2 norm) of a tensor along its first axis.\n",
        "def euclid_norm(x):\n",
        "    return tf.sqrt(tf.reduce_sum(tf.square(x), axis=1))\n",
        "\n",
        "def clr(epoch, step_size=10, base_lr=1e-4, max_lr=1e-2):\n",
        "    cycle = np.floor(1 + epoch / (2 * step_size))\n",
        "    x = np.abs(epoch / step_size - 2 * cycle + 1)\n",
        "    lr = base_lr + (max_lr - base_lr) * np.maximum(0., 1 - x)\n",
        "    return lr # Changed to return a float instead of a TensorFlow tensor\n",
        "\n",
        "#Creates a fully connected neural network layer with optional dropout for regularization.\n",
        "# Estimator Layer class definition\n",
        "class Estimator(keras.layers.Layer):\n",
        "    # Estimation network\n",
        "    def __init__(self, hidden_sizes, activation=\"elu\", kernel_initializer=\"he_normal\", dropout_rate=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        #Initializes a list of dense (fully connected) layers for the hidden layers of the network.\n",
        "        self.hidden = [\n",
        "            keras.layers.Dense(size, activation=activation, kernel_initializer=kernel_initializer,\n",
        "                               kernel_regularizer=keras.regularizers.l2(0.01))\n",
        "            for size in hidden_sizes[:-1]\n",
        "        ]\n",
        "        #Defines the output layer with softmax activation for classification or probability estimation.\n",
        "        self.out = keras.layers.Dense(hidden_sizes[-1], activation=\"softmax\",\n",
        "                                      kernel_initializer=kernel_initializer, kernel_regularizer=keras.regularizers.l2(0.01))\n",
        "        #Initializes a dropout layer to prevent overfitting if dropout_rate is specified.\n",
        "        self.dropout_layer = keras.layers.Dropout(rate=dropout_rate)\n",
        "\n",
        "#Defines the forward pass through the Estimator layer.\n",
        "    def call(self, z):\n",
        "        for layer in self.hidden:\n",
        "            z = layer(z)\n",
        "            if self.dropout_layer.rate:  # Apply dropout only if a dropout rate is specified\n",
        "                z = self.dropout_layer(z)\n",
        "        output = self.out(z)\n",
        "        return output\n",
        "\n",
        "#Implements the MPPCA model with a neural network-based responsibility estimation.\n",
        "class NNMPPCA:\n",
        "    \"\"\"Neural network -based Mixture of Probabilistic Principal Component Analyzers.\n",
        "    \"\"\"\n",
        "\n",
        "    MODEL_FILENAME = \"MPPCA_model\" #Name of the file where the trained model will be saved.\n",
        "    SCALER_FILENAME = \"MPPCA_scaler\" #Name of the file where the scaler (used for normalization) will be saved.\n",
        "\n",
        "    def __init__(self, est_sizes, activation, kernel_initializer, dropout_rate=0.2,\n",
        "                 n_epochs=1000, batch_size = 128, eta=[1, 0.01, 0.001], learning_rate=0.001,\n",
        "                 patience=10, normalize=True, random_seed=42):\n",
        "\n",
        "\n",
        "        inputs = keras.layers.Input(shape=(est_sizes[0],), name=\"input\")\n",
        "        #inputs = keras.layers.Input(shape=(32,), name=\"input\")\n",
        "\n",
        "        #Uses the Estimator class to create a network for responsibility (gamma) estimation.\n",
        "        # Use the Estimator as a layer directly in the model\n",
        "        gamma = Estimator(hidden_sizes=est_sizes[1:],\n",
        "                          activation=activation,\n",
        "                          kernel_initializer=kernel_initializer,\n",
        "                          dropout_rate=dropout_rate)(inputs)\n",
        "\n",
        "        #Creates the MPPCA model where inputs are mapped to gamma (responsibilities).\n",
        "        self.mppca = keras.models.Model(inputs=inputs, outputs=[gamma])\n",
        "\n",
        "        #Stores the hyperparameters for the model and training.\n",
        "        self.eta = eta\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_epochs = n_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.patience = patience\n",
        "        self.seed = random_seed\n",
        "        #Keeps track of the parameters (phi, mu, L) at different training epochs.\n",
        "        self.phi_list = []\n",
        "        self.mu_list = []\n",
        "        self.L_list = []\n",
        "\n",
        "\n",
        "    def parameter_initialize(self, inputs, q=3):\n",
        "\n",
        "        d = inputs.shape[-1]\n",
        "\n",
        "        # Compute responsibilities (gamma) for each component\n",
        "        R = self.mppca(inputs)  # Shape: (n_samples, n_components)\n",
        "\n",
        "        # Calculate total responsibility (gamma_sum) and mixing coefficients (phi)\n",
        "        R_sum = tf.reduce_sum(R, axis=0)  # Shape: (n_components,)\n",
        "        phi = tf.reduce_mean(R, axis=0)  # Shape: (n_components,)\n",
        "\n",
        "        # Compute the mean (mu) for each component\n",
        "        mu = tf.einsum('ik,il->kl', R, inputs) / R_sum[:, np.newaxis]  # Shape: (n_components, n_features)\n",
        "\n",
        "        # Center and weight data using gamma for each component\n",
        "        x_centered_weighted = tf.sqrt(R[:, :, np.newaxis]) * (inputs[:, np.newaxis, :] - mu[np.newaxis, :, :])\n",
        "        # Shape: (n_samples, n_components, n_features)\n",
        "\n",
        "        # Compute covariance matrix (S) for each component\n",
        "        S = tf.einsum('ikl,ikm->klm', x_centered_weighted, x_centered_weighted) / R_sum[:, np.newaxis, np.newaxis]\n",
        "        # Shape: (n_components, n_features, n_features)\n",
        "        S = S + tf.eye(S.shape[-1]) * 1e-6  # Add a small value to the diagonal for regularization\n",
        "        # Eigenvalue decomposition of S and reorder in descending order\n",
        "        eig, vec = tf.linalg.eigh(S)\n",
        "        eig, vec = eig[:, ::-1], vec[:, :, ::-1]\n",
        "\n",
        "        # Estimate noise variance (sigma2) and select top-q eigenvectors (U)\n",
        "        sigma2 = tf.reduce_mean(eig[:, q:], axis=1)  # Shape: (n_components,)\n",
        "        U_q = vec[:, :, :q]  # Top-q eigenvectors (shape: [n_components, n_features, q])\n",
        "        Lambda_q = tf.linalg.diag(eig[:, :q])  # Diagonal matrix of top-q eigenvalues (shape: [n_components, q, q])\n",
        "\n",
        "        # Create the (Λ_q - σ^2 * I) term and take the square root\n",
        "        I_q = tf.eye(q)  # Identity matrix of size q (shape: [q, q])\n",
        "        adjusted_Lambda_q = tf.sqrt(Lambda_q - sigma2[:, tf.newaxis, tf.newaxis] * I_q)  # Shape: [n_components, q, q]\n",
        "\n",
        "        # Multiply U_q by the square root term to get W\n",
        "        W = tf.einsum('ikl,ilm->ikm', U_q, adjusted_Lambda_q)  # Shape: [n_components, n_features, q]\n",
        "\n",
        "        return sigma2, W\n",
        "\n",
        "\n",
        "    def custom_loss(self, inputs, q):\n",
        "\n",
        "        d = inputs.shape[-1]\n",
        "        W_prev = self.W\n",
        "        sigma2_prev = self.sigma2\n",
        "\n",
        "        # Compute responsibilities (gamma) for each component\n",
        "        R = self.mppca(inputs)  # Shape: (n_samples, n_components)\n",
        "\n",
        "        # Calculate total responsibility (gamma_sum) and mixing coefficients (phi)\n",
        "        R_sum = tf.reduce_sum(R, axis=0)  # Shape: (n_components,)\n",
        "        phi = tf.reduce_mean(R, axis=0)  # Shape: (n_components,)\n",
        "\n",
        "        # Compute the mean (mu) for each component\n",
        "        mu = tf.einsum('ik,il->kl', R, inputs) / R_sum[:, np.newaxis]  # Shape: (n_components, n_features)\n",
        "\n",
        "        # Center and weight data using gamma for each component\n",
        "        x_centered_weighted = tf.sqrt(R[:, :, np.newaxis]) * (inputs[:, np.newaxis, :] - mu[np.newaxis, :, :])\n",
        "        # Shape: (n_samples, n_components, n_features)\n",
        "\n",
        "        # Compute covariance matrix (S) for each component\n",
        "        S = tf.einsum('ikl,ikm->klm', x_centered_weighted, x_centered_weighted) / R_sum[:, np.newaxis, np.newaxis]\n",
        "        # Shape: (n_components, n_features, n_features)\n",
        "\n",
        "        # Compute SW, the product of S and W_prev\n",
        "        SW_prev = tf.einsum('ikl,ilm->ikm', S, W_prev)  # Shape: (n_components, n_features, q)\n",
        "\n",
        "        # Create sigma2_prev * I as sigma2I, where I is the identity matrix\n",
        "        I_q = tf.eye(q)  # Identity matrix of size q (shape: [q, q])\n",
        "        sigma2_prevI_q = sigma2_prev[:, np.newaxis, np.newaxis] * I_q  # Shape: (n_components, q, q)\n",
        "\n",
        "        # Compute M and its inverse (Minv)\n",
        "        M = sigma2_prevI_q + tf.einsum('ikl,ilm->ikm', tf.transpose(W_prev, [0, 2, 1]), W_prev)  # Shape: (n_components, q, q)\n",
        "        M_inv = tf.linalg.inv(M)  # Shape: (n_components, q, q)\n",
        "\n",
        "        # Compute Minv * W_prev^T (MinvWT)\n",
        "        M_invW_prevT = tf.einsum('ikl,ilm->ikm', M_inv, tf.transpose(W_prev, [0, 2, 1]))  # Shape: (n_components, q, n_features)\n",
        "\n",
        "        # # Small value added to the diagonal to ensure numerical stability\n",
        "        # min_vals_W = tf.linalg.diag(tf.ones(q, dtype=tf.float32)) * 1e-3  # Shape: (q, q)\n",
        "\n",
        "        # Compute A, used to calculate the new W\n",
        "        middle_term = sigma2_prevI_q + tf.einsum('ikl,ilm->ikm', M_invW_prevT, SW_prev)  # Shape: (n_components, q, q)\n",
        "\n",
        "\n",
        "\n",
        "        # Calculate the updated W (W_new) as SW * Ainv\n",
        "        W_new = tf.einsum('ikl,ilm->ikm', SW_prev, tf.linalg.inv(middle_term))  # Shape: (n_components, n_features, q)\n",
        "\n",
        "        # Compute Minv * W_new^T (MinvWnT)\n",
        "        M_invW_newT = tf.einsum('ikl,ilm->ikm', M_inv, tf.transpose(W_new, [0, 2, 1]))  # Shape: (n_components, q, n_features)\n",
        "\n",
        "        # Calculate the updated sigma2 (sigma2_new) using the trace\n",
        "        sigma2_new = 1 / d * tf.linalg.trace(S - tf.einsum('ikl,ilm->ikm', SW_prev, M_invW_newT))\n",
        "\n",
        "\n",
        "        # model covariance\n",
        "        I_d = tf.eye(d)  # Identity matrix of size q (shape: [q, q])\n",
        "        C = ((sigma2_new[:, np.newaxis, np.newaxis]*I_d) +\n",
        "             tf.einsum('ikl,ilm->ikm', W_new, tf.transpose(W_new, [0, 2, 1])))\n",
        "\n",
        "\n",
        "        # Cholesky decomposition of covariance matrix with regularization for stability\n",
        "        min_vals = I_d * 1e-4            # (d, d)\n",
        "        L = tf.linalg.cholesky(C + min_vals[np.newaxis, :, :])  # L: (n_components, d, d)\n",
        "\n",
        "        # Center input data by subtracting component means\n",
        "        x_centered = inputs[:, np.newaxis, :] - mu[np.newaxis, :, :]  # (n_samples, n_components, d)\n",
        "\n",
        "        # Solve triangular system for Mahalanobis distance\n",
        "        v = tf.linalg.triangular_solve(L, tf.transpose(x_centered, [1, 2, 0]))  # (n_components, d, n_samples)\n",
        "\n",
        "        # Compute log-determinant of covariance from Cholesky factor\n",
        "        log_det_cov = 2.0 * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(L)), axis=1)  # (n_components,)\n",
        "\n",
        "        # Calculate log-probabilities (logits) for each component\n",
        "        logits = tf.math.log(phi[:, np.newaxis]) - 0.5 * (\n",
        "            tf.reduce_sum(tf.square(v), axis=1)\n",
        "            + d * tf.math.log(2.0 * tf.constant(np.pi, dtype=\"float32\"))\n",
        "            + log_det_cov[:, np.newaxis]\n",
        "        )  # logits: (n_components, n_samples)\n",
        "\n",
        "        # Compute negative log-likelihood using log-sum-exp trick\n",
        "        reconstruction_probability = tf.reduce_logsumexp(logits, axis=0)  # (n_samples,)\n",
        "\n",
        "\n",
        "        # Calculate the reconstruction error of MMPCA\n",
        "\n",
        "        # Compute transition matrix and individual component reconstruction in fewer steps\n",
        "        B = tf.einsum('ikl, ilm->ikm', W_new, tf.linalg.inv(tf.einsum('ikl, ilm->ikm', tf.transpose(W_new, [0, 2, 1]), W_new))) \\\n",
        "            @ tf.transpose(W_new, [0, 2, 1])  # Shape: (n_components, d, d)\n",
        "\n",
        "        # Calculate individual component reconstruction of x and overall reconstruction\n",
        "        x_k = tf.einsum('ikl, ilm->ikm', B, tf.transpose(x_centered, [1, 2, 0])) + mu[:, :, tf.newaxis]\n",
        "        x_res = tf.einsum('ik, ikm->im', R, tf.transpose(x_k, [2, 0, 1]))  # Shape: (n_samples, d)\n",
        "\n",
        "        # Compute reconstruction error\n",
        "        reconstruction_error = tf.reduce_sum(tf.square(inputs - x_res), axis=1)\n",
        "\n",
        "\n",
        "       # Extract individual weights (eta_1, eta_2, eta_3) for different loss components\n",
        "        eta_1, eta_2, eta_3 = self.eta[0], self.eta[1], self.eta[2]\n",
        "\n",
        "        # Calculate the penalty term as the sum of the inverses of the diagonal elements of middle_term\n",
        "        penalty_term = tf.reduce_sum(tf.divide(1, tf.linalg.diag_part(middle_term)))\n",
        "\n",
        "        # Compute the total loss as a weighted combination of three components:\n",
        "        # 1. Negative reconstruction probability (multiplied by eta_1)\n",
        "        # 2. Reconstruction error (multiplied by eta_2)\n",
        "        # 3. Penalty term (multiplied by eta_3)\n",
        "\n",
        "        total_loss = (- eta_1 * tf.reduce_mean(reconstruction_probability)\n",
        "                      + eta_2 * tf.reduce_mean(reconstruction_error) + eta_3 * penalty_term)\n",
        "\n",
        "        # Update the model's parameters with the latest values\n",
        "        self.phi = phi             # Mixing coefficient\n",
        "        self.mu = mu               # Mean of each component\n",
        "        self.W = W_new             # Updated transformation matrix\n",
        "        self.sigma2 = sigma2_new   # Updated variance for noise\n",
        "        self.L = L                 # Cholesky factor of the covariance\n",
        "\n",
        "        # Return the computed total loss\n",
        "        return total_loss\n",
        "\n",
        "    def fit(self, inputs, X_test=None, y_test=None):\n",
        "        # Set random seeds for reproducibility\n",
        "        tf.random.set_seed(self.seed)\n",
        "        np.random.seed(seed=self.seed)\n",
        "\n",
        "        # Split the input data into training and validation sets\n",
        "        X_train, X_valid = train_test_split(inputs, test_size=0.3, random_state=42)\n",
        "\n",
        "        # Determine the number of steps per epoch based on the batch size\n",
        "        n_steps = len(X_train) // self.batch_size\n",
        "        # Initialize the optimizer with the specified learning rate\n",
        "        optimizer = keras.optimizers.Nadam(learning_rate=self.learning_rate)\n",
        "\n",
        "        # Initialize metrics to track training and validation loss\n",
        "        mean_loss = keras.metrics.Mean(name='mean_loss')\n",
        "        metrics = keras.metrics.Mean(name='val_loss')\n",
        "        minimum_val_loss = float(\"inf\")  # Keep track of the lowest validation loss\n",
        "        best_epoch = 1                   # Track the best epoch\n",
        "        wait = 0                          # Early stopping counter\n",
        "\n",
        "        # Dimensionality for the latent space\n",
        "        q = 3\n",
        "\n",
        "        # Initialize parameters sigma2 and W based on the training data\n",
        "        self.sigma2, self.W = self.parameter_initialize(X_train, q)\n",
        "\n",
        "        # Start the training loop for the specified number of epochs\n",
        "        for epoch in range(1, self.n_epochs + 1):\n",
        "            print(\"Epoch {}/{}\".format(epoch, self.n_epochs))\n",
        "\n",
        "            # Reset loss metrics at the start of each epoch\n",
        "            mean_loss.reset_state()\n",
        "            metrics.reset_state()\n",
        "\n",
        "            # Iterate over each batch of training data\n",
        "            for step in range(1, n_steps + 1):\n",
        "                # Select a random batch from the training data\n",
        "                X_batch = random_batch(X_train, batch_size=self.batch_size)\n",
        "\n",
        "                # Compute the loss and gradients for the batch\n",
        "                with tf.GradientTape() as tape:\n",
        "                    main_loss = self.custom_loss(X_batch, q)  # Custom loss for the batch\n",
        "                    loss = tf.add_n([main_loss] + self.mppca.losses)  # Add any additional losses\n",
        "                gradients = tape.gradient(loss, self.mppca.trainable_variables)  # Calculate gradients\n",
        "                optimizer.apply_gradients(zip(gradients, self.mppca.trainable_variables))  # Apply gradients\n",
        "\n",
        "                # Ensure any constrained variables remain within their constraints\n",
        "                for variable in self.mppca.variables:\n",
        "                    if variable.constraint is not None:\n",
        "                        variable.assign(variable.constraint(variable))\n",
        "\n",
        "            # Calculate training and validation loss at the end of the epoch\n",
        "            train_loss = tf.add_n([self.custom_loss(X_train, q)] + self.mppca.losses)\n",
        "            mean_loss(train_loss)  # Update mean training loss\n",
        "            val_loss = tf.add_n([self.custom_loss(X_valid, q)] + self.mppca.losses)\n",
        "            metrics(val_loss)  # Update mean validation loss\n",
        "            print_status_bar(len(inputs), len(inputs), mean_loss, [metrics])\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        # Get the dimensionality of the input data\n",
        "        d = X_test.shape[-1]\n",
        "\n",
        "        # Retrieve model parameters at the best epoch for prediction\n",
        "        phi = self.phi_list[self.best_epoch - 1]  # Mixing coefficients for each component\n",
        "        mu = self.mu_list[self.best_epoch - 1]    # Means of each component\n",
        "        L = self.L_list[self.best_epoch - 1]      # Cholesky factor of the covariance matrices\n",
        "\n",
        "        # Center the test data by subtracting the mean of each component\n",
        "        x_centered = X_test[:, np.newaxis, :] - mu[np.newaxis, :, :]  # Shape: (n_samples, n_components, d)\n",
        "\n",
        "        # Solve for the Mahalanobis distance using triangular solve with the Cholesky factor\n",
        "        v = tf.linalg.triangular_solve(L, tf.transpose(x_centered, [1, 2, 0]))  # Shape: (n_components, d, n_samples)\n",
        "\n",
        "        # Calculate the log-determinant of the covariance matrix using the Cholesky factor\n",
        "        log_det_cov = 2.0 * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(L)), axis=1)  # Shape: (n_components,)\n",
        "\n",
        "        # Compute log-probabilities (logits) for each component using the log-sum-exp trick\n",
        "        logits = tf.math.log(phi[:, np.newaxis]) - 0.5 * (\n",
        "            tf.reduce_sum(tf.square(v), axis=1)  # Mahalanobis distance term\n",
        "            + d * tf.math.log(2.0 * tf.constant(np.pi, dtype=\"float32\"))  # Normalization constant\n",
        "            + log_det_cov[:, np.newaxis]  # Log-determinant term\n",
        "        )  # Shape: (n_components, n_samples)\n",
        "\n",
        "        # Use log-sum-exp to compute the reconstruction probabilities for each sample\n",
        "        reconstruction_probabilities = tf.reduce_logsumexp(logits, axis=0)  # Shape: (n_samples,)\n",
        "\n",
        "        # Return the negative reconstruction probabilities as the \"energy\" score\n",
        "        return -reconstruction_probabilities.numpy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp2aGSlNcyeC"
      },
      "source": [
        "## Semi MPPCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKUZR1boodT1"
      },
      "outputs": [],
      "source": [
        "n_classes = 3\n",
        "train,test = train_test_split(df, test_size=0.3,stratify=df['labels'], random_state=None)\n",
        "\n",
        "# Desired split: 100 instances per class for the first dataset\n",
        "instances_per_class = 10\n",
        "# Initialize lists to store the sampled and remaining data\n",
        "sampled_data = []\n",
        "remaining_data = []\n",
        "# Perform sampling for each class\n",
        "for class_label in range(n_classes):\n",
        "    # Subset for the current class\n",
        "    class_subset = train[train['labels'] == class_label]\n",
        "    # Sample the required number of instances\n",
        "    sampled_subset = class_subset.sample(n=instances_per_class, random_state=None)\n",
        "    sampled_data.append(sampled_subset)\n",
        "    # Identify remaining instances\n",
        "    remaining_subset = class_subset.drop(sampled_subset.index)\n",
        "    remaining_data.append(remaining_subset)\n",
        "\n",
        "# Concatenate sampled subsets to form the first dataset\n",
        "dataset_1 = pd.concat(sampled_data)\n",
        "# Concatenate remaining subsets to form the second dataset\n",
        "dataset_2 = pd.concat(remaining_data)\n",
        "# Print the results\n",
        "print(f\"Dataset 1 shape: {dataset_1.shape}\")  # Should have 200 instances (100 per class)\n",
        "print(f\"Dataset 2 shape: {dataset_2.shape}\")  # Remaining instances\n",
        "\n",
        "# Reset indices for clarity\n",
        "train_labeled = dataset_1.reset_index(drop=True)\n",
        "train_unlabeled = dataset_2.reset_index(drop=True)\n",
        "\n",
        "y_train_labeled = train_labeled.labels\n",
        "X_train_labeled= train_labeled.drop([ 'labels'], axis= 1)\n",
        "X_train_labeled = X_train_labeled.to_numpy()\n",
        "y_train_labeled = y_train_labeled.to_numpy()\n",
        "\n",
        "y_train_unlabeled = train_unlabeled.labels\n",
        "X_train_unlabeled= train_unlabeled.drop([ 'labels'], axis= 1)\n",
        "X_train_unlabeled = X_train_unlabeled.to_numpy()\n",
        "y_train_unlabeled = y_train_unlabeled.to_numpy()\n",
        "\n",
        "X_test = test.drop(['labels'], axis= 1)\n",
        "y_test = test.labels\n",
        "X_test = X_test.to_numpy()\n",
        "y_test = y_test.to_numpy()\n",
        "\n",
        "scaler = MinMaxScaler().fit(X_train_unlabeled)\n",
        "# Transform both the training and test sets using the fitted scaler\n",
        "X_train_unlabeled, X_train_labeled = scaler.transform(X_train_unlabeled), scaler.transform(X_train_labeled)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define and initialize the MPPCA model with specific architecture and training parameters\n",
        "nn_mppca = NNMPPCA(est_sizes=[32, 50, 15, 3],\n",
        "                   activation=\"selu\",\n",
        "                   kernel_initializer=\"glorot_normal\",\n",
        "                   dropout_rate=0.2,\n",
        "                   n_epochs=10,\n",
        "                   batch_size=256,\n",
        "                   eta=[1, 0.5, 0.005],\n",
        "                   learning_rate=0.002,\n",
        "                   patience=20)\n",
        "\n",
        "# Fit the MPPCA model to the training data and validate using the test data\n",
        "nn_mppca.fit(X_train_unlabeled)\n",
        "\n",
        "# Creating a copy\n",
        "new_model = tf.keras.models.clone_model(nn_mppca.mppca)\n",
        "new_model.set_weights(nn_mppca.mppca.get_weights())\n",
        "\n",
        "# Freeze the input and hidden layers only\n",
        "for layer in new_model.layers[:-1]:  # Skip the last layer (output layer)\n",
        "    layer.trainable = False\n",
        "\n",
        "# The output layer remains trainable\n",
        "new_model.layers[-1].trainable = True\n",
        "\n",
        "\n",
        "true_labels = y_train_labeled\n",
        "#print(\"true_labels:\", true_labels)\n",
        "pred_labels = tf.argmax(new_model(X_train_labeled), axis=-1)\n",
        "#print(\"pred_labels:\", pred_labels.numpy())\n",
        "\n",
        "matched_pred_labels, cluster_mapping = match_clusters(pred_labels.numpy(), true_labels)\n",
        "#print(\"matched_pred_labels:\", matched_pred_labels)\n",
        "#print(\"cluster_mapping:\", cluster_mapping)\n",
        "\n",
        "# Extract keys and values as separate lists\n",
        "predicted_classes = list(cluster_mapping.keys())\n",
        "true_classes = list(cluster_mapping.values())\n",
        "#print(f\"Cluster Mapping: {cluster_mapping}, predicted_classes: {predicted_classes}\")\n",
        "\n",
        "original_list = predicted_classes.copy()\n",
        "#print(\"original_list:\", original_list)\n",
        "\n",
        "accuracy = accuracy_score(true_labels, matched_pred_labels)\n",
        "#print(\"Accuracy_1:\", accuracy)\n",
        "\n",
        "if predicted_classes == list(range(n_classes)):\n",
        "    print(\"Dont need to swap weights\")\n",
        "else:\n",
        "    print(\"Need to swap weights\")\n",
        "     # Get all the weights of the last layer (estimator_2)\n",
        "    all_weights = new_model.layers[-1].get_weights()\n",
        "    # Assuming the weights you want to modify are at index 4 (last_layer_weights)\n",
        "    last_layer_weights = all_weights[4]\n",
        "    # Swapping columns according to the order\n",
        "    swapped_weights = tf.gather(last_layer_weights, predicted_classes, axis=1)\n",
        "     # Replace the modified weights back into the list\n",
        "    all_weights[4] = swapped_weights\n",
        "    # Update the layer with the complete list of weights\n",
        "    new_model.layers[-1].set_weights(all_weights)\n",
        "\n",
        "\n",
        "# Get model predictions\n",
        "y_pred = new_model.predict(X_train_labeled)\n",
        "# Convert predictions to class labels\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)  # Get the class with the highest probability\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(true_labels, y_pred_labels)\n",
        "print(\"Accuracy_2:\", accuracy)\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',  # Monitors training loss\n",
        "    factor=0.5,      # Reduces learning rate by 50%\n",
        "    patience=10,      # Waits 3 epochs for improvement before reducing learning rate\n",
        "    min_lr=0.002      # Stops reducing once learning rate reaches this value\n",
        ")\n",
        "\n",
        "#Define the unsupervised early stopping\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',  # Track self-supervised loss\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "# Split Train/Validation - take 10 samples for val (where len(X_train_labeled) > 10)\n",
        "test_size = 10 / len(X_train_labeled)\n",
        "\n",
        "X_train_labeled, X_val, y_train_labeled, y_val = train_test_split(\n",
        "    X_train_labeled, y_train_labeled, test_size=test_size, stratify=y_train_labeled, random_state=None\n",
        ")\n",
        "\n",
        "# #Retrain model / finetune with few labeled samples\n",
        "# Compile the new model\n",
        "new_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.05),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "# Train the model with the learning rate scheduler\n",
        "history = new_model.fit(X_train_labeled, y_train_labeled,\n",
        "                validation_data=(X_val, y_val),  # Provide test data for validation\n",
        "                epochs=150,\n",
        "                batch_size=10,\n",
        "                callbacks=[reduce_lr, early_stopping]\n",
        "                        )\n",
        "\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = new_model.predict(X_test)\n",
        "\n",
        "# Get the predicted classes\n",
        "y_pred_flag = np.argmax(y_pred, axis=1)\n",
        "\n",
        "\n",
        "# Compute metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_flag)\n",
        "# Calculate precision, recall, and F1-score using predicted class labels\n",
        "precision = precision_score(y_test, y_pred_flag, average='weighted', zero_division=0)  # Changed to y_pred_flag\n",
        "f1 = f1_score(y_test, y_pred_flag, average='weighted')  # Changed to y_pred_flag\n",
        "recall = recall_score(y_test, y_pred_flag, average='weighted')  # Changed to y_pred_flag\n",
        "\n",
        "y_test = y_test.astype(int)\n",
        "# Convert y_true to one-hot encoding\n",
        "y_true_onehot = np.zeros((len(y_test), len(set(y_test))))\n",
        "y_true_onehot[np.arange(len(y_test)), y_test] = 1\n",
        "# Compute AUC score\n",
        "auc = roc_auc_score(y_true_onehot, y_pred, multi_class=\"ovr\")\n",
        "\n",
        "\n",
        "#Pint the results\n",
        "print(f\"Average Accuracy: {accuracy: .4f}\" )\n",
        "print(f\"Average Precision: {precision: .4f}\")\n",
        "print(f\"Average F1 Score: {f1: .4f}\")\n",
        "print(f\"Average Recall: {recall : .4f}\")\n",
        "print(f\"Average AUC score:{auc: .4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf-myenv",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}